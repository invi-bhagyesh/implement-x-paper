{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuvrajsingh/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "import torchinfo\n",
    "from torch import nn\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path(\"data2/\")\n",
    "# data_path = Path(\"cifar10/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = data_path / \"pizza_steak_sushi\"\n",
    "# image_path = data_path / \"cifar\"\n",
    "train_dir = image_path / \"train\"\n",
    "test_dir = image_path / \"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transforms\n",
    "data_transform = transforms.Compose([\n",
    "    transforms.Resize(size=(224,224)),\n",
    "    # transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.TrivialAugmentWide(num_magnitude_bins=31),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "train_data = datasets.ImageFolder(train_dir, transform=data_transform, target_transform=None)\n",
    "test_data = datasets.ImageFolder(test_dir, transform=data_transform, target_transform=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(dataset=train_data, num_workers=os.cpu_count(), batch_size=32, shuffle=True)\n",
    "test_dataloader = DataLoader(dataset=test_data, num_workers=os.cpu_count(), batch_size=32, shuffle=True)\n",
    "# train = torchvision.datasets.CIFAR10(root=image_path, train=True, download=True, transform=data_transform)\n",
    "# test = torchvision.datasets.CIFAR10(root=image_path, train=False, download=True, transform=data_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset ImageFolder\n",
       "    Number of datapoints: 225\n",
       "    Root location: data2/pizza_steak_sushi/train\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=True)\n",
       "               TrivialAugmentWide(num_magnitude_bins=31, interpolation=InterpolationMode.NEAREST, fill=None)\n",
       "               ToTensor()\n",
       "           )"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset ImageFolder\n",
       "    Number of datapoints: 75\n",
       "    Root location: data2/pizza_steak_sushi/test\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=True)\n",
       "               TrivialAugmentWide(num_magnitude_bins=31, interpolation=InterpolationMode.NEAREST, fill=None)\n",
       "               ToTensor()\n",
       "           )"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Patch Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbeddings(nn.Module):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 in_channels: int=3, \n",
    "                 embeddings_dimensions: int=768,\n",
    "                 patch_size: int=16):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.patch_size = patch_size\n",
    "        self.patched_embeddings = nn.Conv2d(in_channels=in_channels, out_channels=embeddings_dimensions, stride=patch_size, padding=0, kernel_size=patch_size)\n",
    "        \n",
    "        self.flatten_embeddings = nn.Flatten(start_dim=2, end_dim=3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        image_resolution = x.shape[-1]\n",
    "        assert image_resolution % self.patch_size == 0, f\"Input image size must be divisible by patch size, image shape: {image_resolution}, patch size: {self.patch_size}\"\n",
    "\n",
    "        x_patched = self.patched_embeddings(x)\n",
    "        x_flatten = self.flatten_embeddings(x_patched)\n",
    "        return x_flatten.permute(0,2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttentionBlock(nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        num_heads: int=12,\n",
    "        embeddings_dimension: int=768,\n",
    "        attn_dropout: int=0\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layer_norm = nn.LayerNorm(embeddings_dimension)\n",
    "        self.multihead_attn_layer = nn.MultiheadAttention(embed_dim=embeddings_dimension, num_heads=num_heads, dropout=attn_dropout, batch_first=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.layer_norm(x)\n",
    "        attn_output, _  = self.multihead_attn_layer(query=x, key=x, value=x, need_weights=False)\n",
    "        # print(attn_output)\n",
    "        return attn_output\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPBlock(nn.Module):\n",
    "  def __init__(self,\n",
    "               embeddings_dimension:int=768,\n",
    "               mlp_size:int=3072,\n",
    "               dropout:int=0.1):\n",
    "    super().__init__()\n",
    "  \n",
    "\n",
    "    self.layer_norm = nn.LayerNorm(normalized_shape=embeddings_dimension)\n",
    "\n",
    "\n",
    "    self.mlp = nn.Sequential(\n",
    "        nn.Linear(in_features=embeddings_dimension,\n",
    "                  out_features=mlp_size),\n",
    "        nn.GELU(),\n",
    "        nn.Dropout(p=dropout),\n",
    "        nn.Linear(in_features=mlp_size,\n",
    "                  out_features=embeddings_dimension),\n",
    "        nn.Dropout(p=dropout) \n",
    "    )\n",
    "  \n",
    "  def forward(self, x):\n",
    "    x = self.layer_norm(x) \n",
    "    x = self.mlp(x)\n",
    "    return x\n",
    "    # return self.mlp(self.layer_norm(x)) # same as above "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransfornmerEncoderBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_heads: int=12,\n",
    "        embeddings_dimension: int=768,\n",
    "        dropout: int=0.1,\n",
    "        mlp_size:  int=3072,\n",
    "        attn_dropout: int=0\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.msa_layer = MultiHeadSelfAttentionBlock(num_heads=num_heads, embeddings_dimension=embeddings_dimension, attn_dropout=attn_dropout)\n",
    "        \n",
    "        self.mlp_block = MLPBlock(dropout=dropout, embeddings_dimension=embeddings_dimension, mlp_size=mlp_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.msa_layer(x) + x\n",
    "        x = self.mlp_block(x) + x\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = TransfornmerEncoderBlock()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# torch_transformer_encoder_layer = nn.TransformerEncoderLayer(d_model=768, # embedding size from table 1\n",
    "#                                                              nhead=12, # heads from table 1\n",
    "#                                                              dim_feedforward=3072, # MLP size from table\n",
    "#                                                              dropout=0.1,\n",
    "#                                                              activation=\"gelu\",\n",
    "#                                                              batch_first=True,\n",
    "#                                                              norm_first=True)\n",
    "\n",
    "# torch_transformer_encoder_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=======================================================================================================================================\n",
       "Layer (type (var_name))                                 Input Shape          Output Shape         Param #              Trainable\n",
       "=======================================================================================================================================\n",
       "TransfornmerEncoderBlock (TransfornmerEncoderBlock)     [1, 197, 768]        [1, 197, 768]        --                   True\n",
       "├─MultiHeadSelfAttentionBlock (msa_layer)               [1, 197, 768]        [1, 197, 768]        --                   True\n",
       "│    └─LayerNorm (layer_norm)                           [1, 197, 768]        [1, 197, 768]        1,536                True\n",
       "│    └─MultiheadAttention (multihead_attn_layer)        --                   [1, 197, 768]        2,362,368            True\n",
       "├─MLPBlock (mlp_block)                                  [1, 197, 768]        [1, 197, 768]        --                   True\n",
       "│    └─LayerNorm (layer_norm)                           [1, 197, 768]        [1, 197, 768]        1,536                True\n",
       "│    └─Sequential (mlp)                                 [1, 197, 768]        [1, 197, 768]        --                   True\n",
       "│    │    └─Linear (0)                                  [1, 197, 768]        [1, 197, 3072]       2,362,368            True\n",
       "│    │    └─GELU (1)                                    [1, 197, 3072]       [1, 197, 3072]       --                   --\n",
       "│    │    └─Dropout (2)                                 [1, 197, 3072]       [1, 197, 3072]       --                   --\n",
       "│    │    └─Linear (3)                                  [1, 197, 3072]       [1, 197, 768]        2,360,064            True\n",
       "│    │    └─Dropout (4)                                 [1, 197, 768]        [1, 197, 768]        --                   --\n",
       "=======================================================================================================================================\n",
       "Total params: 7,087,872\n",
       "Trainable params: 7,087,872\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 4.73\n",
       "=======================================================================================================================================\n",
       "Input size (MB): 0.61\n",
       "Forward/backward pass size (MB): 8.47\n",
       "Params size (MB): 18.90\n",
       "Estimated Total Size (MB): 27.98\n",
       "======================================================================================================================================="
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(model=encoder,\n",
    "        input_size=(1, 197, 768), # (batch_size, number_of_patches, embedding_d\\imension)\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary(model=torch_transformer_encoder_layer,\n",
    "#         input_size=(1, 197, 768), # (batch_size, number_of_patches, embedding_dimension)\n",
    "#         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "#         col_width=20,\n",
    "#         row_settings=[\"var_names\"])\n",
    "     \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_heads: int=12,\n",
    "        embeddings_dimension: int=768,\n",
    "        dropout: int=0.1,\n",
    "        mlp_size:  int=3072,\n",
    "        attn_dropout: int=0,\n",
    "        num_of_encoder_layers: int=12,\n",
    "        patch_size: int=16,\n",
    "        image_width: int=224,\n",
    "        img_height: int=224,\n",
    "        no_channels: int=3,\n",
    "        classes: int=1000,\n",
    "        positional_embedding_dropout: int=0.1\n",
    "        \n",
    "    ):\n",
    "        assert (img_height * image_width) % patch_size == 0\n",
    "        \n",
    "        super().__init__()\n",
    "        self.number_of_patches = (image_width * img_height)//(patch_size * patch_size)\n",
    "        # print(self.number_of_patches\n",
    "        self.patch_embeddings = PatchEmbeddings(in_channels=no_channels, embeddings_dimensions=embeddings_dimension,patch_size=patch_size)\n",
    "        self.positional_embeddings = nn.Parameter(torch.randn(1, self.number_of_patches + 1, embeddings_dimension), requires_grad=True)\n",
    "        self.cls_token = nn.Parameter(torch.randn(1,1, embeddings_dimension), requires_grad=True)\n",
    "        \n",
    "        # self.encoder_layer = TransfornmerEncoderBlock(num_heads=num_heads, embeddings_dimension=embeddings_dimension, dropout=dropout, mlp_size=mlp_size,attn_dropout=attn_dropout)\n",
    "        \n",
    "        self.encoder_block = nn.Sequential(*[TransfornmerEncoderBlock(num_heads=num_heads, embeddings_dimension=embeddings_dimension, dropout=dropout, mlp_size=mlp_size,attn_dropout=attn_dropout) for _ in range(num_of_encoder_layers)])\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            \n",
    "            nn.LayerNorm(embeddings_dimension),\n",
    "            nn.Linear(in_features=embeddings_dimension, out_features=classes)\n",
    "        )\n",
    "        \n",
    "        self.dropout_after_positional_embeddings = nn.Dropout(p=positional_embedding_dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        x = self.patch_embeddings(x)\n",
    "        prepend_token = self.cls_token.expand(batch_size, -1, -1) \n",
    "        \n",
    "        x = torch.cat((prepend_token, x), dim=1)\n",
    "        x = self.positional_embeddings + x\n",
    "        x = self.dropout_after_positional_embeddings(x)\n",
    "        x = self.encoder_block(x)\n",
    "        x = self.classifier(x[:,0])\n",
    "        \n",
    "        return x\n",
    "            \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViT(\n",
       "  (patch_embeddings): PatchEmbeddings(\n",
       "    (patched_embeddings): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "    (flatten_embeddings): Flatten(start_dim=2, end_dim=3)\n",
       "  )\n",
       "  (encoder_block): Sequential(\n",
       "    (0): TransfornmerEncoderBlock(\n",
       "      (msa_layer): MultiHeadSelfAttentionBlock(\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (multihead_attn_layer): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (mlp_block): MLPBlock(\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): TransfornmerEncoderBlock(\n",
       "      (msa_layer): MultiHeadSelfAttentionBlock(\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (multihead_attn_layer): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (mlp_block): MLPBlock(\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): TransfornmerEncoderBlock(\n",
       "      (msa_layer): MultiHeadSelfAttentionBlock(\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (multihead_attn_layer): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (mlp_block): MLPBlock(\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): TransfornmerEncoderBlock(\n",
       "      (msa_layer): MultiHeadSelfAttentionBlock(\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (multihead_attn_layer): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (mlp_block): MLPBlock(\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (4): TransfornmerEncoderBlock(\n",
       "      (msa_layer): MultiHeadSelfAttentionBlock(\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (multihead_attn_layer): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (mlp_block): MLPBlock(\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (5): TransfornmerEncoderBlock(\n",
       "      (msa_layer): MultiHeadSelfAttentionBlock(\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (multihead_attn_layer): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (mlp_block): MLPBlock(\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (6): TransfornmerEncoderBlock(\n",
       "      (msa_layer): MultiHeadSelfAttentionBlock(\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (multihead_attn_layer): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (mlp_block): MLPBlock(\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (7): TransfornmerEncoderBlock(\n",
       "      (msa_layer): MultiHeadSelfAttentionBlock(\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (multihead_attn_layer): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (mlp_block): MLPBlock(\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (8): TransfornmerEncoderBlock(\n",
       "      (msa_layer): MultiHeadSelfAttentionBlock(\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (multihead_attn_layer): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (mlp_block): MLPBlock(\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (9): TransfornmerEncoderBlock(\n",
       "      (msa_layer): MultiHeadSelfAttentionBlock(\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (multihead_attn_layer): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (mlp_block): MLPBlock(\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (10): TransfornmerEncoderBlock(\n",
       "      (msa_layer): MultiHeadSelfAttentionBlock(\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (multihead_attn_layer): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (mlp_block): MLPBlock(\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (11): TransfornmerEncoderBlock(\n",
       "      (msa_layer): MultiHeadSelfAttentionBlock(\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (multihead_attn_layer): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (mlp_block): MLPBlock(\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (1): Linear(in_features=768, out_features=3, bias=True)\n",
       "  )\n",
       "  (dropout_after_positional_embeddings): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vit = ViT(classes=3)\n",
    "vit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================================================================================================\n",
       "Layer (type (var_name))                                           Input Shape          Output Shape         Param #              Trainable\n",
       "=================================================================================================================================================\n",
       "ViT (ViT)                                                         [1, 3, 224, 224]     [1, 3]               152,064              True\n",
       "├─PatchEmbeddings (patch_embeddings)                              [1, 3, 224, 224]     [1, 196, 768]        --                   True\n",
       "│    └─Conv2d (patched_embeddings)                                [1, 3, 224, 224]     [1, 768, 14, 14]     590,592              True\n",
       "│    └─Flatten (flatten_embeddings)                               [1, 768, 14, 14]     [1, 768, 196]        --                   --\n",
       "├─Dropout (dropout_after_positional_embeddings)                   [1, 197, 768]        [1, 197, 768]        --                   --\n",
       "├─Sequential (encoder_block)                                      [1, 197, 768]        [1, 197, 768]        --                   True\n",
       "│    └─TransfornmerEncoderBlock (0)                               [1, 197, 768]        [1, 197, 768]        --                   True\n",
       "│    │    └─MultiHeadSelfAttentionBlock (msa_layer)               [1, 197, 768]        [1, 197, 768]        2,363,904            True\n",
       "│    │    └─MLPBlock (mlp_block)                                  [1, 197, 768]        [1, 197, 768]        4,723,968            True\n",
       "│    └─TransfornmerEncoderBlock (1)                               [1, 197, 768]        [1, 197, 768]        --                   True\n",
       "│    │    └─MultiHeadSelfAttentionBlock (msa_layer)               [1, 197, 768]        [1, 197, 768]        2,363,904            True\n",
       "│    │    └─MLPBlock (mlp_block)                                  [1, 197, 768]        [1, 197, 768]        4,723,968            True\n",
       "│    └─TransfornmerEncoderBlock (2)                               [1, 197, 768]        [1, 197, 768]        --                   True\n",
       "│    │    └─MultiHeadSelfAttentionBlock (msa_layer)               [1, 197, 768]        [1, 197, 768]        2,363,904            True\n",
       "│    │    └─MLPBlock (mlp_block)                                  [1, 197, 768]        [1, 197, 768]        4,723,968            True\n",
       "│    └─TransfornmerEncoderBlock (3)                               [1, 197, 768]        [1, 197, 768]        --                   True\n",
       "│    │    └─MultiHeadSelfAttentionBlock (msa_layer)               [1, 197, 768]        [1, 197, 768]        2,363,904            True\n",
       "│    │    └─MLPBlock (mlp_block)                                  [1, 197, 768]        [1, 197, 768]        4,723,968            True\n",
       "│    └─TransfornmerEncoderBlock (4)                               [1, 197, 768]        [1, 197, 768]        --                   True\n",
       "│    │    └─MultiHeadSelfAttentionBlock (msa_layer)               [1, 197, 768]        [1, 197, 768]        2,363,904            True\n",
       "│    │    └─MLPBlock (mlp_block)                                  [1, 197, 768]        [1, 197, 768]        4,723,968            True\n",
       "│    └─TransfornmerEncoderBlock (5)                               [1, 197, 768]        [1, 197, 768]        --                   True\n",
       "│    │    └─MultiHeadSelfAttentionBlock (msa_layer)               [1, 197, 768]        [1, 197, 768]        2,363,904            True\n",
       "│    │    └─MLPBlock (mlp_block)                                  [1, 197, 768]        [1, 197, 768]        4,723,968            True\n",
       "│    └─TransfornmerEncoderBlock (6)                               [1, 197, 768]        [1, 197, 768]        --                   True\n",
       "│    │    └─MultiHeadSelfAttentionBlock (msa_layer)               [1, 197, 768]        [1, 197, 768]        2,363,904            True\n",
       "│    │    └─MLPBlock (mlp_block)                                  [1, 197, 768]        [1, 197, 768]        4,723,968            True\n",
       "│    └─TransfornmerEncoderBlock (7)                               [1, 197, 768]        [1, 197, 768]        --                   True\n",
       "│    │    └─MultiHeadSelfAttentionBlock (msa_layer)               [1, 197, 768]        [1, 197, 768]        2,363,904            True\n",
       "│    │    └─MLPBlock (mlp_block)                                  [1, 197, 768]        [1, 197, 768]        4,723,968            True\n",
       "│    └─TransfornmerEncoderBlock (8)                               [1, 197, 768]        [1, 197, 768]        --                   True\n",
       "│    │    └─MultiHeadSelfAttentionBlock (msa_layer)               [1, 197, 768]        [1, 197, 768]        2,363,904            True\n",
       "│    │    └─MLPBlock (mlp_block)                                  [1, 197, 768]        [1, 197, 768]        4,723,968            True\n",
       "│    └─TransfornmerEncoderBlock (9)                               [1, 197, 768]        [1, 197, 768]        --                   True\n",
       "│    │    └─MultiHeadSelfAttentionBlock (msa_layer)               [1, 197, 768]        [1, 197, 768]        2,363,904            True\n",
       "│    │    └─MLPBlock (mlp_block)                                  [1, 197, 768]        [1, 197, 768]        4,723,968            True\n",
       "│    └─TransfornmerEncoderBlock (10)                              [1, 197, 768]        [1, 197, 768]        --                   True\n",
       "│    │    └─MultiHeadSelfAttentionBlock (msa_layer)               [1, 197, 768]        [1, 197, 768]        2,363,904            True\n",
       "│    │    └─MLPBlock (mlp_block)                                  [1, 197, 768]        [1, 197, 768]        4,723,968            True\n",
       "│    └─TransfornmerEncoderBlock (11)                              [1, 197, 768]        [1, 197, 768]        --                   True\n",
       "│    │    └─MultiHeadSelfAttentionBlock (msa_layer)               [1, 197, 768]        [1, 197, 768]        2,363,904            True\n",
       "│    │    └─MLPBlock (mlp_block)                                  [1, 197, 768]        [1, 197, 768]        4,723,968            True\n",
       "├─Sequential (classifier)                                         [1, 768]             [1, 3]               --                   True\n",
       "│    └─LayerNorm (0)                                              [1, 768]             [1, 768]             1,536                True\n",
       "│    └─Linear (1)                                                 [1, 768]             [1, 3]               2,307                True\n",
       "=================================================================================================================================================\n",
       "Total params: 85,800,963\n",
       "Trainable params: 85,800,963\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 172.47\n",
       "=================================================================================================================================================\n",
       "Input size (MB): 0.60\n",
       "Forward/backward pass size (MB): 102.88\n",
       "Params size (MB): 229.20\n",
       "Estimated Total Size (MB): 332.69\n",
       "================================================================================================================================================="
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model=vit,\n",
    "        input_size=(1, 3, 224, 224), # (batch_size, number_of_patches, embedding_dimension)\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"])\n",
    "     \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from going_modular.going_modular import engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(weight_decay=0.03, params=vit.parameters(), lr=3e-3)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:33<05:02, 33.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 1.1293 | train_acc: 0.2578 | test_loss: 1.1456 | test_acc: 0.2178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [01:09<04:41, 35.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 | train_loss: 1.0806 | train_acc: 0.4414 | test_loss: 1.1424 | test_acc: 0.2377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [01:46<04:09, 35.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 | train_loss: 1.0869 | train_acc: 0.3867 | test_loss: 1.1460 | test_acc: 0.1979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [02:21<03:34, 35.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 | train_loss: 1.1563 | train_acc: 0.2734 | test_loss: 1.1342 | test_acc: 0.2576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [02:57<02:58, 35.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 | train_loss: 1.1286 | train_acc: 0.2656 | test_loss: 1.1447 | test_acc: 0.2178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [03:34<02:23, 35.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 | train_loss: 1.1419 | train_acc: 0.2852 | test_loss: 1.1375 | test_acc: 0.2377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [04:10<01:48, 36.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 | train_loss: 1.0961 | train_acc: 0.4492 | test_loss: 1.1253 | test_acc: 0.2775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [04:46<01:12, 36.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8 | train_loss: 1.0957 | train_acc: 0.4219 | test_loss: 1.1404 | test_acc: 0.2377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [05:22<00:35, 35.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9 | train_loss: 1.1612 | train_acc: 0.3008 | test_loss: 1.1196 | test_acc: 0.2775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [05:58<00:00, 35.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 | train_loss: 1.1447 | train_acc: 0.3164 | test_loss: 1.1248 | test_acc: 0.2775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results = engine.train(model=vit,\n",
    "                       train_dataloader=train_dataloader,\n",
    "                       test_dataloader=test_dataloader,\n",
    "                       optimizer=optimizer,\n",
    "                       loss_fn=loss_fn,\n",
    "                       epochs=10,\n",
    "                       device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
